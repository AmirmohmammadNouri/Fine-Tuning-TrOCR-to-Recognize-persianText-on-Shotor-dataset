{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d7047ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    " \n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from urllib.request import urlretrieve\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfd77b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    " \n",
    "seed_everything(42)\n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3171ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "94c99dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:    int = 48\n",
    "    EPOCHS:        int = 15\n",
    "    LEARNING_RATE: float = 0.00005\n",
    " \n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    DATA_ROOT:     str = 'C:/Users/AmirNouri/Desktop/font_project/Shotor/custom_shotor/'\n",
    " \n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    MODEL_NAME: str = 'microsoft/trocr-small-printed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ee1a7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize(dataset_path):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(15):\n",
    "        plt.subplot(3, 5, i+1)\n",
    "        all_images = os.listdir(f\"{dataset_path}/shotor_train\")\n",
    "        image = plt.imread(f\"{dataset_path}/shotor_train/{all_images[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(all_images[i].split('.')[0])\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "visualize(DatasetConfig.DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e690f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_fwf(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'shotor_train.txt'), header=None\n",
    ")\n",
    "train_df.rename(columns={0: 'file_name', 1: 'text'}, inplace=True)\n",
    "test_df = pd.read_fwf(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'shotor_test.txt'), header=None\n",
    ")\n",
    "test_df.rename(columns={0: 'file_name', 1: 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "425ea078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241.jpg</td>\n",
       "      <td>سر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>242.jpg</td>\n",
       "      <td>داد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>243.jpg</td>\n",
       "      <td>جان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244.jpg</td>\n",
       "      <td>هر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245.jpg</td>\n",
       "      <td>باشد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>246.jpg</td>\n",
       "      <td>هیچ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>247.jpg</td>\n",
       "      <td>خوش</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>248.jpg</td>\n",
       "      <td>با</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>249.jpg</td>\n",
       "      <td>پس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>250.jpg</td>\n",
       "      <td>داد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>251.jpg</td>\n",
       "      <td>گفت</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>252.jpg</td>\n",
       "      <td>شد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>253.jpg</td>\n",
       "      <td>جان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>254.jpg</td>\n",
       "      <td>سخن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>255.jpg</td>\n",
       "      <td>چشم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>256.jpg</td>\n",
       "      <td>چنین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>257.jpg</td>\n",
       "      <td>دم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>258.jpg</td>\n",
       "      <td>چو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>259.jpg</td>\n",
       "      <td>آتش</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>260.jpg</td>\n",
       "      <td>خود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>261.jpg</td>\n",
       "      <td>سوی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>262.jpg</td>\n",
       "      <td>شب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>263.jpg</td>\n",
       "      <td>باد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>264.jpg</td>\n",
       "      <td>این</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>265.jpg</td>\n",
       "      <td>چشم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>266.jpg</td>\n",
       "      <td>بی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>267.jpg</td>\n",
       "      <td>کز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>268.jpg</td>\n",
       "      <td>گل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>269.jpg</td>\n",
       "      <td>هر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>270.jpg</td>\n",
       "      <td>دو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>271.jpg</td>\n",
       "      <td>تا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>272.jpg</td>\n",
       "      <td>یکی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>273.jpg</td>\n",
       "      <td>همی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>274.jpg</td>\n",
       "      <td>همی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>275.jpg</td>\n",
       "      <td>ای</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>276.jpg</td>\n",
       "      <td>چه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>277.jpg</td>\n",
       "      <td>کار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>278.jpg</td>\n",
       "      <td>یک</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>279.jpg</td>\n",
       "      <td>این</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>280.jpg</td>\n",
       "      <td>بس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>281.jpg</td>\n",
       "      <td>در</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>282.jpg</td>\n",
       "      <td>و</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>283.jpg</td>\n",
       "      <td>این</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>284.jpg</td>\n",
       "      <td>کسی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>285.jpg</td>\n",
       "      <td>تا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>286.jpg</td>\n",
       "      <td>دگر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>287.jpg</td>\n",
       "      <td>بهر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>288.jpg</td>\n",
       "      <td>شاه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>289.jpg</td>\n",
       "      <td>چون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>290.jpg</td>\n",
       "      <td>دست</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>291.jpg</td>\n",
       "      <td>سوی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>292.jpg</td>\n",
       "      <td>نیست</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>293.jpg</td>\n",
       "      <td>زین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>294.jpg</td>\n",
       "      <td>یار</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>295.jpg</td>\n",
       "      <td>هم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>296.jpg</td>\n",
       "      <td>چشم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>297.jpg</td>\n",
       "      <td>خون</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>298.jpg</td>\n",
       "      <td>جان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>299.jpg</td>\n",
       "      <td>نیست</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>300.jpg</td>\n",
       "      <td>آن</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  text\n",
       "0    241.jpg    سر\n",
       "1    242.jpg   داد\n",
       "2    243.jpg   جان\n",
       "3    244.jpg    هر\n",
       "4    245.jpg  باشد\n",
       "5    246.jpg   هیچ\n",
       "6    247.jpg   خوش\n",
       "7    248.jpg    با\n",
       "8    249.jpg    پس\n",
       "9    250.jpg   داد\n",
       "10   251.jpg   گفت\n",
       "11   252.jpg    شد\n",
       "12   253.jpg   جان\n",
       "13   254.jpg   سخن\n",
       "14   255.jpg   چشم\n",
       "15   256.jpg  چنین\n",
       "16   257.jpg    دم\n",
       "17   258.jpg    چو\n",
       "18   259.jpg   آتش\n",
       "19   260.jpg   خود\n",
       "20   261.jpg   سوی\n",
       "21   262.jpg    شب\n",
       "22   263.jpg   باد\n",
       "23   264.jpg   این\n",
       "24   265.jpg   چشم\n",
       "25   266.jpg    بی\n",
       "26   267.jpg    کز\n",
       "27   268.jpg    گل\n",
       "28   269.jpg    هر\n",
       "29   270.jpg    دو\n",
       "30   271.jpg    تا\n",
       "31   272.jpg   یکی\n",
       "32   273.jpg   همی\n",
       "33   274.jpg   همی\n",
       "34   275.jpg    ای\n",
       "35   276.jpg    چه\n",
       "36   277.jpg   کار\n",
       "37   278.jpg    یک\n",
       "38   279.jpg   این\n",
       "39   280.jpg    بس\n",
       "40   281.jpg    در\n",
       "41   282.jpg     و\n",
       "42   283.jpg   این\n",
       "43   284.jpg   کسی\n",
       "44   285.jpg    تا\n",
       "45   286.jpg   دگر\n",
       "46   287.jpg   بهر\n",
       "47   288.jpg   شاه\n",
       "48   289.jpg   چون\n",
       "49   290.jpg   دست\n",
       "50   291.jpg   سوی\n",
       "51   292.jpg  نیست\n",
       "52   293.jpg   زین\n",
       "53   294.jpg   یار\n",
       "54   295.jpg    هم\n",
       "55   296.jpg   چشم\n",
       "56   297.jpg   خون\n",
       "57   298.jpg   جان\n",
       "58   299.jpg  نیست\n",
       "59   300.jpg    آن"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "442a3de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000.jpg</td>\n",
       "      <td>ترا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001.jpg</td>\n",
       "      <td>جان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002.jpg</td>\n",
       "      <td>روز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003.jpg</td>\n",
       "      <td>گر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004.jpg</td>\n",
       "      <td>نه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236.jpg</td>\n",
       "      <td>چنین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237.jpg</td>\n",
       "      <td>چشم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238.jpg</td>\n",
       "      <td>روی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239.jpg</td>\n",
       "      <td>سر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240.jpg</td>\n",
       "      <td>جز</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    file_name  text\n",
       "0     000.jpg   ترا\n",
       "1     001.jpg   جان\n",
       "2     002.jpg   روز\n",
       "3     003.jpg    گر\n",
       "4     004.jpg    نه\n",
       "..        ...   ...\n",
       "236   236.jpg  چنین\n",
       "237   237.jpg   چشم\n",
       "238   238.jpg   روی\n",
       "239   239.jpg    سر\n",
       "240   240.jpg    جز\n",
       "\n",
       "[241 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e2f2d603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0fd487a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    " \n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    " \n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # The image file name.\n",
    "        file_name = self.df['file_name'][idx]\n",
    "        # The text (label).\n",
    "        text = self.df['text'][idx]\n",
    "        # Read the image, apply augmentations, and get the transformed pixels.\n",
    "        image = Image.open(self.root_dir + file_name).convert('RGB')\n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        # i.e. tokenized labels.\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b51145c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'shotor_train/'),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'shotor_test/'),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b6b4c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.41 GiB already allocated; 0 bytes free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(ModelConfig\u001b[38;5;241m.\u001b[39mMODEL_NAME)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Total parameters and trainable parameters.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\modeling_utils.py:2065\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2061\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2062\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2063\u001b[0m     )\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (3 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.41 GiB already allocated; 0 bytes free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4a02a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Set Correct vocab size.\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    " \n",
    " \n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "237ace3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "24e1da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = evaluate.load('cer')\n",
    " \n",
    " \n",
    "def compute_cer(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    " \n",
    " \n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    " \n",
    " \n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    " \n",
    " \n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "58942737",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    output_dir='seq2seq_model_printed/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='tensorboard',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e098b709",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.40 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize trainer.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_cer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_data_collator\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer_seq2seq.py:56\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m     model: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedModel\u001b[39m\u001b[38;5;124m\"\u001b[39m, nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     55\u001b[0m ):\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer.py:506\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[0;32m    505\u001b[0m ):\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer.py:730\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[1;34m(self, model, device)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[1;32m--> 730\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\modeling_utils.py:2065\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2061\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2062\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2063\u001b[0m     )\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 797 (3 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.40 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Initialize trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_cer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0614e80",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 4.00 GiB total capacity; 3.38 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_summary(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, abbreviated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1841\u001b[0m ):\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer.py:2679\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2679\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2682\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\trainer.py:2704\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2703\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2704\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2706\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\accelerate\\utils\\operations.py:636\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\accelerate\\utils\\operations.py:624\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\accelerate\\utils\\operations.py:636\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\accelerate\\utils\\operations.py:624\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 624 (1 times), autocast_decorator.<locals>.decorate_autocast at line 14 (1 times), convert_outputs_to_fp32.<locals>.forward at line 636 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\accelerate\\utils\\operations.py:636\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\accelerate\\utils\\operations.py:624\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:581\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[1;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 581\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    589\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\u001b[38;5;241m*\u001b[39mencoder_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:530\u001b[0m, in \u001b[0;36mDeiTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    526\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m    528\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[1;32m--> 530\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:373\u001b[0m, in \u001b[0;36mDeiTEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    367\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    368\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    369\u001b[0m         hidden_states,\n\u001b[0;32m    370\u001b[0m         layer_head_mask,\n\u001b[0;32m    371\u001b[0m     )\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:311\u001b[0m, in \u001b[0;36mDeiTLayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    307\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    308\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 311\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in DeiT, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    317\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:250\u001b[0m, in \u001b[0;36mDeiTAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    246\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    247\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    248\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 250\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\embolismenv\\lib\\site-packages\\transformers\\models\\deit\\modeling_deit.py:173\u001b[0m, in \u001b[0;36mDeiTSelfAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    170\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 4.00 GiB total capacity; 3.38 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc83543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a482850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779692cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
